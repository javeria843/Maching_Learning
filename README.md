# ğŸš€ Machine Learning Practice â€“ Titanic Dataset & Ensemble Learning  

This repository contains my hands-on practice with **Machine Learning models** using the famous **Titanic Dataset**.  
The goal is to predict passenger survival and explore how different ML models and **Ensemble Learning techniques** (Bagging & Boosting) improve performance.  

---

## ğŸ“Œ Features
- Data preprocessing with **Pandas** & **Scikit-learn**  
- Train-test split & feature scaling  
- Implementation of ML models (Logistic Regression, Decision Tree, etc.)  
- **Ensemble Learning**:
  - âœ… Bagging (Random Forest Classifier)  
  - âœ… Boosting (AdaBoost Classifier)  

---

## ğŸ“‚ Repository Structure
ğŸ“¦ Machine_Learning
â”£ ğŸ“œ Lab_Task_ML.ipynb # Titanic dataset preprocessing & model training
â”£ ğŸ“œ Decision_Tree_classifier.ipynb
â”£ ğŸ“œ Ensemble_learning.ipynb # Bagging & Boosting implementation
â”£ ğŸ“œ lab03_simple_linear_regression.ipynb
â”£ ğŸ“œ Lab_3_Multivariate_Linear_Regression.ipynb

---

## âš™ï¸ How to Run
1. Clone the repo:
   ```bash
   git clone https://github.com/javeria843/Maching_Learning.git
   cd Maching_Learning
ğŸ· Key Concepts

Bagging (Bootstrap Aggregating):
Trains multiple models on random subsets of data in parallel to reduce variance. Example: Random Forest ğŸŒ²

Boosting:
Trains models sequentially, each correcting errors of the previous one to reduce bias. Example: AdaBoost âš¡

ğŸ“¢ Connect with Me

If youâ€™re interested in ML & Data Science, letâ€™s connect on LinkedIn
https://www.linkedin.com/in/javeriaiqbalai/
